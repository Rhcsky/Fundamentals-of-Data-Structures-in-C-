{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "모두를 위한 RL 강좌 6, Deep Q-learning",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "489e9438387c44afb80a1c961df8f159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_716ec78b6fe64396b743ba3a370435ff",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_40d34bd07f2741b2ac0ad12dcccbd7d2",
              "IPY_MODEL_b954df49357e474daf1f244616a112d1"
            ]
          }
        },
        "716ec78b6fe64396b743ba3a370435ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40d34bd07f2741b2ac0ad12dcccbd7d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3ebae5de188f4709a7dc30d2e5af026a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3faac8552d00441f8a33803016204407"
          }
        },
        "b954df49357e474daf1f244616a112d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ec9dca3d01ad4897850c1bfe58592145",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2000/2000 [00:26&lt;00:00, 75.63it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6b530ef95654b8baeba78e780bea2b5"
          }
        },
        "3ebae5de188f4709a7dc30d2e5af026a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3faac8552d00441f8a33803016204407": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec9dca3d01ad4897850c1bfe58592145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6b530ef95654b8baeba78e780bea2b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO9NCS-9M25h",
        "colab_type": "text"
      },
      "source": [
        "# Q-function Approximation\n",
        "\n",
        "input : State <br>\n",
        "output : Q-value(right,left,up,down) -> **all action**<br>\n",
        "y_label = Q, Ws = Q_prediction\n",
        "\n",
        "$$cost(W) =  (Ws-y)^2$$\n",
        "\n",
        "$$ y = r + \\gamma max Q(s`) $$\n",
        "$$\\hat Q(s,a|\\theta) = Q^*(s,a)$$\n",
        "\n",
        "### But Is Q* converges?\n",
        " No!, diverges using neural networds due to\n",
        " * Correlations between samples\n",
        " * Non-stationary targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjYbKyiKQ714",
        "colab_type": "text"
      },
      "source": [
        "Input = One-Hot-incoding\n",
        "```python\n",
        "def one_hot(x):\n",
        "  np.identity(16)[x:x+1]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnutG_XNUKdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6ee9cb29-f973-470f-fbfd-acb26b22715a"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVaHcusSWSSj",
        "colab_type": "text"
      },
      "source": [
        "# FrozenLake-v0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBqQM-pEMJLn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484,
          "referenced_widgets": [
            "489e9438387c44afb80a1c961df8f159",
            "716ec78b6fe64396b743ba3a370435ff",
            "40d34bd07f2741b2ac0ad12dcccbd7d2",
            "b954df49357e474daf1f244616a112d1",
            "3ebae5de188f4709a7dc30d2e5af026a",
            "3faac8552d00441f8a33803016204407",
            "ec9dca3d01ad4897850c1bfe58592145",
            "b6b530ef95654b8baeba78e780bea2b5"
          ]
        },
        "outputId": "cc754d40-4fde-4047-83ff-20824067e7f9"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "# Input and output size based on the Env\n",
        "input_size = env.observation_space.n;\n",
        "output_size = env.action_space.n;\n",
        "learning_rate = 0.1\n",
        "\n",
        "# These lines establish the feed-forward part of the network used to choose actions\n",
        "X = tf.placeholder(shape=[1, input_size], dtype=tf.float32)              # state input\n",
        "W = tf.get_variable(\"w\",shape=[input_size, output_size],\n",
        "                    initializer = tf.contrib.layers.xavier_initializer())   # weight\n",
        "\n",
        "Qpred = tf.matmul(X, W)     # Out Q prediction\n",
        "Y = tf.placeholder(shape=[1, output_size], dtype=tf.float32)    # Y label\n",
        "\n",
        "loss = tf.reduce_sum(tf.square(Y-Qpred))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "# Set Q-learning parameters\n",
        "dis = .99\n",
        "num_episodes = 2000\n",
        "\n",
        "# create lists to contain total rewards and steps per episode\n",
        "rList = []\n",
        "\n",
        "def one_hot(x):\n",
        "    return np.identity(16)[x:x+1]\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for i in tqdm(range(num_episodes)):\n",
        "        # Reset environment and get first new observation\n",
        "        s = env.reset()\n",
        "        e = 1. / ((i / 50) + 10)\n",
        "        rAll = 0\n",
        "        done = False\n",
        "        local_loss = []\n",
        "\n",
        "        # The Q-Table learning algorithm\n",
        "        while not done:\n",
        "            # Choose an action by greedly (with a chance of random action)\n",
        "            # from the Q-network\n",
        "            Qs = sess.run(Qpred, feed_dict={X: one_hot(s)})\n",
        "            if np.random.rand(1) < e:\n",
        "                a = env.action_space.sample()\n",
        "            else:\n",
        "                a = np.argmax(Qs)\n",
        "\n",
        "            # Get new state and reward from environment\n",
        "            s1, reward, done, _ = env.step(a)\n",
        "            if done:\n",
        "                # Update Q, and no Qs+1, since it's a termial state\n",
        "                Qs[0, a] = reward\n",
        "            else:\n",
        "                # Obtain the Q_s` values by feeding the new state through our network\n",
        "                Qs1 = sess.run(Qpred, feed_dict={X: one_hot(s1)})\n",
        "                # Update Q\n",
        "                Qs[0, a] = reward + dis*np.max(Qs1)\n",
        "\n",
        "            # Train our network using target (Y) and predicted Q (Qpred) values\n",
        "            sess.run(train, feed_dict={X: one_hot(s), Y: Qs})\n",
        "\n",
        "            rAll += reward\n",
        "            s = s1\n",
        "\n",
        "        rList.append(rAll)\n",
        "\n",
        "print(\"Success rate: \" + str(sum(rList) / num_episodes))\n",
        "plt.bar(range(len(rList)), rList, color='b', alpha=0.4)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "489e9438387c44afb80a1c961df8f159",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Success rate: 0.024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgSMtxVZWVC5",
        "colab_type": "text"
      },
      "source": [
        "# CartPole-v0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ahvyc5_WX8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "random_episodes = 0\n",
        "reward_sum = 0\n",
        "\n",
        "while random_episodes < 10:\n",
        "  env.render()\n",
        "  action = env.action_space.sample()\n",
        "  s1,reward,done,_ = env.step(action)\n",
        "\n",
        "  if done:\n",
        "    Qs[0,a] = -100\n",
        "  else:\n",
        "    x1 = np.reshape(s1,[1,input_size])\n",
        "    Qs1 = sess.run(Qpred,feed_dict={X:x1})\n",
        "    Qs[0,a] = reward + dis*np.max(Qs1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H7SJZI6fgQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# Constants defining our neural network\n",
        "learning_rate = 1e-1\n",
        "input_size = env.observation_space.shape[0];\n",
        "output_size = env.action_space.n;\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, input_size], name=\"input_x\" )\n",
        "\n",
        "# First layer of weights\n",
        "W1 = tf.get_variable(\"W1\", shape=[input_size, output_size], initializer=tf.contrib.layers.xavier_initializer() )   # weight\n",
        "\n",
        "Qpred = tf.matmul(X, W1)\n",
        "\n",
        "# We need to define the parts of the network needed for learning a policy\n",
        "Y = tf.placeholder(shape=[1, output_size], dtype=tf.float32)    # Y label\n",
        "\n",
        "# Loss function\n",
        "loss = tf.reduce_sum(tf.square(Y-Qpred))\n",
        "\n",
        "# Learning\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "# Values for q-learning\n",
        "dis = .99\n",
        "num_episodes = 2000\n",
        "rList = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for i in range(num_episodes):\n",
        "        e = 1. / ((i / 10) + 1)\n",
        "        rAll = 0\n",
        "        step_count = 0\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "\n",
        "        # The Q-Table learning algorithm\n",
        "        while not done:\n",
        "            step_count += 1\n",
        "            x = np.reshape(s, [1, input_size])\n",
        "            # Choose an action by greedly (with a chance of random action) from the Q-network\n",
        "            Qs = sess.run(Qpred, feed_dict={X: x})\n",
        "            if np.random.rand(1) < e:\n",
        "                a = env.action_space.sample()\n",
        "            else:\n",
        "                a = np.argmax(Qs)\n",
        "\n",
        "            # Get new state and reward from environment\n",
        "            s1, reward, done, _ = env.step(a)\n",
        "            if done:\n",
        "                # Update Q, and no Qs+1, since it's a termial state\n",
        "                Qs[0, a] = -100\n",
        "            else:\n",
        "                x1 = np.reshape(s1, [1, input_size])\n",
        "                # Obtain the Q_s` values by feeding the new state through our network\n",
        "                Qs1 = sess.run(Qpred, feed_dict={X: x1})\n",
        "                Qs[0, a] = reward + dis*np.max(Qs1)\n",
        "\n",
        "            # Train our network using target (Y) and predicted Q (Qpred) values\n",
        "            sess.run(train, feed_dict={X: x, Y: Qs})\n",
        "            s = s1\n",
        "\n",
        "        rList.append(step_count)\n",
        "        print(\"Episode: {} steps: {}\".format(i, step_count))\n",
        "        # If last 10's avg steps are 500, It's good enough\n",
        "        if len(rList) > 10 and np.mean(rList[-10:]) > 500:\n",
        "            break\n",
        "\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "    # See our trained network in action\n",
        "    observation = env.reset()\n",
        "    reward_sum = 0\n",
        "    while True:\n",
        "        env.render()\n",
        "\n",
        "        x = np.reshape(observation, [1, input_size])\n",
        "        Qs = sess.run(Qpred, feed_dict={X: x})\n",
        "        a = np.argmax(Qs)\n",
        "\n",
        "        observation, reward, done, _ = env.step(a)\n",
        "        reward_sum += reward\n",
        "        if done:\n",
        "            print(\"Total score: {}\".format(reward_sum))\n",
        "            break\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}