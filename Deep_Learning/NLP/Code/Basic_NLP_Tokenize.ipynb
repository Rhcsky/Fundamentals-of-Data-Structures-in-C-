{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic NLP - Tokenize",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1JRcrO1ySC42j964nLSyc90eEflhXlUm0",
      "authorship_tag": "ABX9TyPUfWcbNAW7ZMhpp73kEj1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rhcsky/Slef_Study/blob/master/Deep-Learning/NLP/code/Basic_NLP_Tokenize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nvh-_xFXQBY",
        "colab_type": "text"
      },
      "source": [
        "## Install Mecab & Konlpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeQOy27jXT27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY3-GBucXZV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd Mecab-ko-for-Google-Colab/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_U-Pc0vXaVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffCEs4PnXiQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEnkHwVrfa7O",
        "colab_type": "text"
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JioBO69XhiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hannanum, Kkma, Komoran, Mecab, Twitter\n",
        "from konlpy.tag import Mecab\n",
        "tagger = Mecab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRul-bLEa3i7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text1 = \"아저씨가방에들어갔다\"\n",
        "ex1 = tagger.pos(text1)\n",
        "\n",
        "ex1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPMmyELafnfu",
        "colab_type": "text"
      },
      "source": [
        "## Read text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv56h76zftcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU9SK11Hfxyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/train_origin.csv')\n",
        "data.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgmduvkokNn5",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDMk1zkTgAY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx, text in enumerate(tqdm(data.text)):\n",
        "  token = tagger.pos(text)\n",
        "  tokenlist = [w[0] for w in token]\n",
        "  # tokenlist = [w for w in token if all([must not in w for must in ['Alpha','X']])]\n",
        "  data.text[idx] = \" \".join(tokenlist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz3TUj3djmdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.to_csv('/content/drive/My Drive/Colab Notebooks/data/tokenizing/train_mecab.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T42aftEgn49a",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing For Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycr6L4ntq-Yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def space_tokenize(text):\n",
        "  text = text.strip()\n",
        "  if not text:\n",
        "    return []\n",
        "  space = text.split()\n",
        "  return space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXufZWM0ofMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/train_origin.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwWQ6eWxsXr",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOJG6XRBxsBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_tokenize(text):\n",
        "  temp_list = []\n",
        "  for space_token in space_tokenize(text):\n",
        "    cnt = 0\n",
        "    for token in tagger.morphs(space_token):\n",
        "      tk = token\n",
        "      if cnt > 0:\n",
        "        tk = \"##\" + tk\n",
        "        temp_list.append(tk)\n",
        "      else:\n",
        "        cnt += 1\n",
        "        temp_list.append(tk)\n",
        "  return temp_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sL8w0jqx5Af",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx, text in enumerate(tqdm(data.text)):\n",
        "  token_list = bert_tokenize(text)\n",
        "  data.text[idx] = \" \".join(token_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcHB18jjz_rv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.to_csv('/content/drive/My Drive/Colab Notebooks/data/tokenizing/train_mecab_bert.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG66xndaxoGs",
        "colab_type": "text"
      },
      "source": [
        "### Make Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X4uwL0vn9ew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_tokens = ['[PAD]','[UNK]','[CLS]','[SEP]','[MASK]']\n",
        "for text in tqdm(data.text):\n",
        "  for space_token in space_tokenize(text):\n",
        "    cnt = 0\n",
        "    for token in tagger.morphs(space_token):\n",
        "      tk = token\n",
        "      if cnt > 0:\n",
        "        tk = \"##\" + tk\n",
        "        if tk in output_tokens: continue\n",
        "        output_tokens.append(tk)\n",
        "      else:\n",
        "        cnt += 1\n",
        "        if tk in output_tokens: continue\n",
        "        output_tokens.append(tk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51RpOQ-6quF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint as pp\n",
        "pp(output_tokens[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SKIgbBHvGsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt_file = open(\"dir\")\n",
        "\n",
        "for token in output_tokens:\n",
        "  txt_file.write(token + '\\n')\n",
        "\n",
        "txt_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}